# -*- coding: utf-8 -*-
"""MNIST.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nYpjhFxx2vpaGU_moUZm_Mj414XSjlwG
"""

import pandas as pd
import numpy as np
from PIL import Image
import io

#load my data "MNIST" form Hugging Face

# 1. You have Parquet-format data hosted on Hugging Face (hf://)
# - One file for training
# - One file for testing
splits = {
    'train': 'mnist/train-00000-of-00001.parquet',
    'test': 'mnist/test-00000-of-00001.parquet'
}

# 2. Load the training dataset by combining the base path with the "train" file
df = pd.read_parquet("hf://datasets/ylecun/mnist/" + splits["train"])

#make function convert this images into numpy array
def images_to_array(img_column):
  byte_data = img_column['image']['bytes']                    #give column to it df.iloc[]
  one_img =Image.open(io.BytesIO(byte_data)).convert('L')     # grayscale [one value from 3 @equation (x,y,z)-->(O)]
  arr_one_img = np.array(one_img).astype('float32') / 255.0   # normalize data to values (0-1)
  return arr_one_img.flatten()                                #make flatten to this image data from (x,x) to (x^2, ) = one row

#test the function
#arr1=images_to_array(df.iloc[0])


#ower final data for training the network
x_train = np.stack(df.apply(images_to_array, axis=1))
y_train= df['label'].values.astype('int')
#make ower network

from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense
Module = Sequential([
    Dense(128,activation='relu',input_shape=(784,)),  #note this is first hidden layer of network
    Dense(64,activation='relu'),
    Dense(10,activation='softmax')
])

Module.summary()

Module.compile(
    optimizer='adam',  #most common use now is adam
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']   #indicator to module performance

)

Module.fit(
    x_train,
    y_train,
    epochs=100,
    batch_size=196,
    validation_split=.2
)
Module.save('MNIST.h5')

#load the test data and prepart to evaluate the mdule .
df2 = pd.read_parquet("hf://datasets/ylecun/mnist/" + splits["test"])
x_test = np.stack(df2.apply(images_to_array, axis=1))
y_test= df2['label'].values.astype('int')


loss , accuracy=Module.evaluate(x_test,y_test)
print(f"the module accuracy is : {accuracy:.4f}")

import matplotlib.pyplot as plt

#pick randomly 10 data points.
np.random.seed(10)
index=np.random.randint(500,10000,size=10)
sample_images = x_test[index]
actual_labels = y_test[index]

#make pridictions for these data points
predict_labels_ = Module.predict(sample_images)
predict_labels = np.argmax(predict_labels_,axis=1)

#plot the results
fig, axes  = plt.subplots(2,5,figsize=(12,6))

for i,ax in enumerate(axes.flat):
  ax.imshow(sample_images[i].reshape(28,28),cmap='gray')
  ax.set_title(f'actual:{actual_labels[i]},predict:{predict_labels[i]}')

plt.tight_layout()
plt.show()

print(accuracy)

